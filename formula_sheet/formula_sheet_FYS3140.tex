\documentclass[a4paper, 10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage{textcomp, xcolor, amssymb, tikz, subfig, float,esint}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{bm}
\usepackage{gensymb}
%\DeclareMathOperator{\arcsinh}{arcsinh}
%\DeclareMathOperator{\arctanh}{arctanh}

%Makes \diff into a straight d in math mode
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand*\Diff[1]{\mathop{}\!\mathrm{d^#1}}

%can box titles
\usepackage{mdframed}

\usepackage{physics}
\usepackage{tikz}
\usepackage{pgfplots}

%\newcommand{\addPLOT}[4]{
%\addplot [domain=#1:#2,samples=200,color=#3,]{#4};}
%\newcommand{\addCOORDS}[1]{\addplot coordinates {#1};}
%\newcommand{\addDRAW}[1]{\draw #1;}
%\newcommand{\addNODE}[2]{ \node at (#1) {#2};}

%		\PLOTS{x}{y}{left}{
%			\ADDPLOT{x^2}{-2}{2}{blue}
%			\ADDCOORDS{(0,1)(1,1)(1,2)}
%		}




%\definecolor{svar}{RGB}{0,0,0}
%\definecolor{opgavetekst}{RGB}{109,109,109}
%\definecolor{blygraa}{RGB}{44,52,59}

\hoffset = -60pt
\voffset = -95pt
\oddsidemargin = 0pt
\topmargin = 0pt
\textheight = 0.97\paperheight
\textwidth = 0.97\paperwidth

\begin{document}
\tiny
\begin{multicols*}{3}
\begin{mdframed}
\subsection*{Complex analysis}
\end{mdframed}
\subsubsection*{Definitions}
\begin{align*}
  \sin{z} &= \frac{1}{2i}\left(e^{iz}-e^{-iz}\right)\quad
  \cos{z} = \frac{1}{2}\left(e^{iz}+e^{-iz}\right)\\
  \sinh{z} &= \frac{1}{2}\left(e^{z}-e^{-z}\right)\qquad
  \cosh{z} = \frac{1}{2}\left(e^{z}+e^{-z}\right)
\end{align*}

\subsubsection*{Roots}
\begin{align*}
  \left(e^{i\theta}\right)^n &= \left(\cos{\theta} + i \sin{\theta})^n\right) = \cos{n\theta} + i\sin{n\theta}\\
  z^{1/n} &= \left(re^{i\theta}\right)^{1/n} = r^{1/n}e^{i\theta/n}.\\
  \ln{z} &= \ln{re^{i\theta}} = \ln{r} + i\theta, \quad \theta \in [0, 2\pi]
\end{align*}


\subsubsection*{Complex series}
\textbf{Comparison test:}If $\abs{z_n} \leq a_n$ and $\sum a_n$ converges, then $\sum z_n$ converges.\\
\textbf{Ratio test:} If $\abs{\frac{z_{n+1}}{z_n}} \leq k$ for all $n$ sufficiently large, and $k<1$, then $\sum z_n$ converges absolutely.\\
\textbf{Divergence check:} If $z_n$ does not converge towards zero, then $\sum z_n$ diverges; the complex and imaginary part will diverge seperately.
\textbf{Complex power series:} The ratio test gives convergence for $\abs{z-z_0} < \abs{\frac{a_n}{a_{n+1}}} = R$ as $n\rightarrow \infty$. We call $R$ radius of convergence, and $\abs{z-z_0} < R$ for the disk of convergence.

\subsubsection*{Cauchy-Riemann equations}
\textbf{Analytic:} $\leftrightarrow$ Has a unique derivative at wanted region. If a function is analytic it has unique derivatives of all orders and is a solution of Laplace's equation.\\
\textbf{Regular point:} A point where $f(z)$ is analytic.\\
\textbf{Singular point} or \textbf{Singularity:} A point where $f(z)$ is not analytic.\\
\textbf{Isolated singularity:} If $f(z)$ is analytic in a small circle around, but not at, the given point.\\
CR-eq. is a tool to check if a function $f(z) = u(x,y) +iv(x,y)$ is analytic in a region by requiring existence of a unique derivative
\begin{align*}
  \pdv{u}{x} &= \pdv{v}{y} \qquad\quad \pdv{v}{x} = - \pdv{u}{y} \\
  \pdv{u}{r} &= \frac{1}{r}\pdv{v}{\theta} \qquad \frac{1}{r}\pdv{u}{\theta} = - \pdv{v}{r}
\end{align*}
must be satisfied in that region.\\
If a function is analytic within some region it can be expanded as a Taylor series about any point $z_0$ inside the region. The power series converges inside a circle about $z_0$ that extends to the nearest singular point.\\
\textbf{Harmonic functions:} Are solutions to the 2D Laplace equation $\nabla^2\phi = 0$. If a function $f(z) = u(x,y) +iv(x,y)$ is \textsc{analytic} in a region, then $u$ and $v$ are harmonic functions.\\
\textbf{Harmonic conjugate:} Given a harmonic function $u(x,y)$, there exists another harmonic functions $v(x,y)$ such that $u+iv$ is an analytic function of $z$ in that region; $v$ is called the \textsc{Harmonic conjugate}.
\begin{itemize}
  \item Check that $u(x,y)$ is harmonic
  \item Find harmonic conjugate through Cauchy-Riemann equations through integration
  \item Express $u+iv$ in terms of $z$
\end{itemize}
\subsubsection*{Integrals of complex functions}
\textbf{Contours:} Finite sequence of directed smooth curves patched together.\\
\textbf{Simple closed contours:} A contour which does not cross itself.\\
\textbf{Positively oriented contour:} A contour with interior to the left and exterior to the right, arrow going counter clockwise.\\
\textbf{Loop:} A closed contour.\\
\textbf{Contour integral:}
\begin{equation*}
  \int_{\Gamma} f(z) \diff z = \lim_{n\to\infty} \sum_{k=1}^{n} f(z_k) \Delta z_k.
\end{equation*}
If a function is continuous in a domain then
\begin{itemize}
  \item $f$ has an anti-derivative
  \item Contour integrals are independent of path
  \item Any loop integral is zero
\end{itemize}
If any of these are true, the two others are true.\\
\begin{equation*}
  \int_{C} (z-z_0)^n \diff z = 2\pi i \delta_{n, -1}.
\end{equation*}
Inside a simply connected region (no singularities) we continuously deform contours without changing the integral of analytic functions along these curves.\\
\textbf{Cauchy-Integral formula:}
For an function $f$, which is analytic inside a simply connected region containing a contour $\Gamma$ which is simple, closed and positively oriented, then:
\begin{equation*}
  f(z_0) = \frac{1}{2\pi i} \int_{\Gamma} \frac{f(z)}{z-z_0} \diff z
\end{equation*}
\begin{equation*}
  f(z_0)^{(n)} = \frac{n!}{2\pi i} \int_{\Gamma} \frac{f(z)}{(z-z_0)^{n+1}} \diff z, n\geq 1
\end{equation*}
\textbf{Liouville's theorem:} A bounded analytic function in the entire complex plane is a constant.\\
\subsubsection*{\scriptsize Upper bound estimates}
Use the generalized triangle inequality
\begin{align*}
  \abs{\sum_k z_k } \leq \sum_k \abs{z_k}\quad \rightarrow \quad
  \abs{z_1-z_2} \geq \abs{z_2}-\abs{z_1}.
\end{align*}
Apply this to Riemann sum
\begin{align*}
  \abs{\int_\Gamma f(z) \diff z} = \abs{\sum_{k=1}^{n} f(z_k) \Delta z_k} \leq \sum_{k=1}^{n} \abs{f(z_k)} \abs{\Delta z_k} \leq M \sum_{k=1}^{n} \abs{\Delta z_k},
\end{align*}
where $M$ is the maximum value of $\abs{f(z)}$ on \textsc{contour}. We then use that $\sum_k \abs{\Delta z_k}$ can not be longer than the length of the contour $L$:
\begin{equation*}
  \abs{\int_\Gamma f(z) \diff z} \leq ML.
\end{equation*}
\textbf{Cauchy inequality:} A function $f$ which is analytic on and inside a circle with radius $R$ centred at $z_0$ satisfy $\abs{f^{(n)}} \leq \frac{n!M}{R^n}$.
\subsubsection*{Laurent series}
Let $f$ be analytic in the area between two circles $r < \abs{z-z_0} < R$, then $f$ can be represented uniquely as the sum of two series
\begin{align*}
  f(z) &= \sum_{n=0}^{\infty} a_n(z-z_0)^n + \sum_{n=1}^{\infty} \frac{b_n}{(z-z_0)^n} \\
  a_n &= \frac{1}{2\pi i} \oint_C \frac{f(z)}{(z-z_0)^{n+1}} \diff z \\
  b_n &= \frac{1}{2\pi i} \oint_C \frac{f(z)}{(z-z_0)^{-n+1}} \diff z
\end{align*}
Derive $a_n$ from using important integral and divide expression for Laurent series with $(z-z_0)^{n+1}$ and integrating. Derive $b_n$ with same method, but multiplying with $(z-z_0)^{n-1}$ instead.\\
Series of positive powers converges inside  some circle $\abs{z-z_0} < R$. \\
Series of negative powers converges outside some circle $\abs{z-z_0} > R$. \\
Rember to write every term a power of $(z-z_0)$ where $z_0$ is the point you are expanding around.
Often useful to use \textsc{partial fraction decomposition} and
\begin{equation*}
  \frac{1}{1-w} = \sum_{n=0}^{\infty} w^n, \quad \abs{w}<1.
\end{equation*}
\textbf{Residue:} Is the value of the coefficient $b_1$.\\
\textbf{A zero of a function:} A point $z_0$ where $f$ is analytic and $f(z_0)=0$.\\
\textbf{A zero of order m:} When $f(z_0) = f^{(1)}(z_0) = ... = f^{(m-1)}(z_0)=0$ and $f^(m)(z_0)\neq 0$. This can be factorized as $f(z) = (z-z_0)^m\,g(z)$, where $g(z)$ is analytic and non-zero at $z_0$.\\
\textbf{Isolated singularities:} Assume $f(z)$ has an isolated singularity at $z_0$, with a Laurent series, then:
\begin{itemize}
  \item \textbf{Removable singularity:} If all $b_n=0$
  \item \textbf{Pole of order m:} If $b_m\neq 0$, but $b_k=0$ for all $k>m$.
  \item \textbf{Essential singularity:} If there are infinitely many $b$-terms
\end{itemize}

\subsubsection*{Residue theory}
If $\Gamma$ is a simple closed, positively oriented contour, and $f$ is analytic on and inside $\Gamma$, except at the points $z_1,...z_k$ inside $\Gamma$, then
\begin{equation}
  \oint_{\Gamma} f(z) \diff z = 2\pi i\, \sum_{k=1}^{N} \text{Res}(f;z_k).
\end{equation}
True since only $1/(z-z_0)$ term contributes in integrals, we get no other contributions sice we can deform contour around singularities such that the path between each singularity cancels. A function $f(z)$ can have a singularity in infinity if $f(1/z)$ has a pole in $z=0$, their residues are the same.
\textbf{Finding residues:}
\begin{itemize}
  \item Find Laurent series around $z_0$, then Res$(f;z_0)$ is $b_1$.
  \item Evaluate Res$(f;z_0)=\lim_{b\rightarrow z_0} \left[f(z)\,(z-z_0)\right]$, finite answer only if it is a simple pole. Removable singularity gives zero, higher order poles gives infinity.
  \item For $f(z)=P(z)/Q(z)$ where $P(z_0)\neq 0$ and $Q(z_0)$ is a \textsc{simple zero}, and both analytic at $z_0$, then Res$(f;z_0)=P(z_0)/Q'(z_0)$.
  \item If $f$ has a pole of order $m$ at $z_0$ then Res$(f;z_0) = \lim_{z\rightarrow z_0} \frac{1}{(M-1)!}\frac{\diff^{M-1}}{\diff z^{M-1}}\left[(z-z_0)^Mf(z)\right]$ where $M \geq m$. If you know the order of the pole us $m=M$, but get correct result by overshooting as well.
\end{itemize}
\subsubsection*{Solving integrals}
\begin{itemize}
\item \textbf{Trigonometric integrals:} Integrals of type $\int_0^{2\pi} u(\cos{\theta}, \sin{\theta}) \diff \theta$, can be made into complex integral by variable substitution $z=e^{i\theta}$ and integrate around $\abs{z}=1$. Use complex version of $\cos{\theta} = (z+1/z)/2$ and $\sin{\theta}=(z-1/z)/2i$ and $\diff \theta = \diff z/(iz)$. Solve the integral with residue theory and take the real value of the final answer.
If the integration limits are not $0$ to $2\pi$ you can use a substitution to change integral limits such that the final limits are $0$ to $2\pi$, f.ex $u=2\pi-\theta$ will change limits from $0\rightarrow\pi$ to $\pi\rightarrow 2\pi$.
\item \textbf{Infinite integrals:} Integrals from $-\infty\rightarrow\infty$ can be extended to the complex plane by connecting $\pm\infty$ at the $x$-axis by a semi-circle with infinite radius giving no contribution. Thus the integral can be solved from residues inside the first and second quadrant, or third and fourth quadrant depending on orientation of semi-circle. Only works if: (1) $f$ is analytic on and above the real axis, except for a finite number of singularities, (2) we can ignore contributions infinitely far away from the origin, we can do this ($f(z)=P(z)/Q(z)$) if $\text{DEG}(Q) \geq \text{DEG}(P)+2$.
\item \textbf{Infinite Trigonometric integral:} Two options:\\1. Write the trigonometric function in it's complex form, which will give two integrals, one with $e^{imx}$ which must be closed in the upper half plane, and one with $e^{-imx}$ which must be closed in the lower half plane. Since the last one goes counter clockwise we must flip the sign. We can ignore the semi-circle only if the degree of the polynomial in the denominator is one larger than the degree of the numerator, this is called \textbf{Jordan's lemma}. Without the exponential the difference in polynomial degree would be two. \\2. If the integral is real we can write $\cos{mx}$ as the real part of $e^{imx}$, or $\sin{mx}$ as the imaginary part, then solve the integral and take the imaginary or real part of the answer.
\item \textbf{Singularities on the real axis:} Infinities can \textit{cancel} if approached symmetrically: PV$\int_a^b f(x)\diff x = \lim_{r\rightarrow 0} \int_a^{c-r} f(x)\diff x + \int_{c+r}^{b}f(x)\diff x$, with a singularity at $x=c$. Can also be computed with residue theory:
$$\text{PV}\int_{-\infty}^{\infty} f(x)\diff x = 2\pi i \sum \underbrace{\text{Res}(f; z_k)}_{\text{upper half}} + \pi i \sum \underbrace{\text{Res}(f;z_j)}_{\text{real axis}}$$.
\end{itemize}
\newpage
\begin{mdframed}
\subsection*{Tensors}
\end{mdframed}
Tensors of rank 0 = scalars, rank 1 = vectors, rank 2 = matrix. Tensors represent physical quantities, and physics is independent of choice of coordinate frames, so tensors must transform between coordinate frames. A tensor product is given by
\[
\left(\begin{matrix}
U_1 \\ U_2 \\ U_3 \\
\end{matrix}\right)
\otimes
\left(\begin{matrix}
V_1 \\ V_2 \\ V_3 \\
\end{matrix}\right)
=
\left(\begin{matrix}
U_1V_1 & U_1V_2 & U_1V_3 \\
U_2V_1 & U_2V_2 & U_2V_3 \\
U_3V_1 & U_3V_2 & U_3V_3 \\
\end{matrix}\right)
\]
Det$(A^T)=$Det$(A)$, and Det($AB)=$Det$(A)$Det$(B)$.\\
For a $3\times 3$ matrix: $\epsilon_{ijk}\det{A} = A_{i\alpha}A_{j\beta}A_{k\gamma}\epsilon_{\alpha\beta\gamma}$.
\subsubsection*{Cartesian tensors}
Transform properly under rotation of a cartesian coordinate
$v_i' = A_{ij}v_j$, $v_i = A_{ji}v_j$, where $A_{ij} = \bm{e}_i\cdot \bm{e}_j'$, which implies $A^{-1} = A^{T}$. We get one $A$ for each index  $T_{kl}' = A_{ki}A_{lj}T_{ij}$. Cartesian vectors need only one index. Tricks:\\
$A_{l\beta}A_{li} = A_{\beta l}^TA_{li} = A_{\beta l}^{-1}A_{li} = \delta_{\beta i}$
\subsubsection*{Inertia tensor}
A rigid body rotating around a fixed axis: $\bm{L}=I\bm{\omega}$, where $I$ is the inertia tensor. Since $I$ is symmetric we can find a coordinate system where $I$ is diagonal. The eigenvectors of $I$ are the principle axis of inertia. \\
\textbf{Point masses:} $I_{ij} = mr^2\delta_{ij}-mr_ir_j$ where you sum over all masses.\\
\textbf{Continuum masses:} $I_{ij} = \int mr^2\delta_{ij}-mr_ir_j \diff m$ where. Examples:\\
$I_{xx} = \sum_i m_i(r^2 -x^2) = \sum_i m_i(y^2 + z^2) = \int y^2 + z^2 \diff m$.\\
$I_{xy} = -\sum_{i}m_ix_iy_i = - \int xy \diff m$.
\[
[\mathbf{I}]=\left[\begin{matrix}
I_{xx} & I_{xy} & I_{xz} \\
I_{yx} & I_{yy} & I_{yz} \\
I_{zx} & I_{zy} & I_{zz} \\
\end{matrix}\right]
=m\left[\begin{matrix}
y^2+z^2 & -xy & -xz \\
-yx & x^2+z^2 & -yz \\
-zx & -zy & x^2+y^2 \\
\end{matrix}\right]
\]
\subsubsection*{Kronecker-delta and Levi-Civita}

\[
\delta_{ij}=\begin{cases}
1 & \mbox{ if } i=j \\
0 & \mbox{ if } i\neq j
\end{cases}\]
\[
\epsilon_{ijk}=\begin{cases}
1 & \mbox{ if } ijk=123,231, \mbox{ or } 312 \\
-1 & \mbox{ if } ijk=321,213, \mbox{ or } 132 \\
0 & \mbox{ if repeating indices} \\
\end{cases}
\]
\[
\epsilon_{ijk}\epsilon_{imn}=\delta_{jm}\delta_{kn}-\delta_{jn}\delta_{km}
\]
$$(\mathbf{a}\times \mathbf{b})_i = a_j b_k \epsilon_{ijk}$$
\[
\mbox{det}\left(\begin{matrix}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33}
\end{matrix}\right)=a_{1i}a_{2j}a_{3k}\epsilon_{ijk}
\]

$$ \left( \bm{\nabla} \cross \bm{V}\right)_i = \epsilon_{ijk}\pdv{V_k}{x_j}$$
$$ \left( \bm{U} \cross \bm{V}\right)_i = \epsilon_{ijk}U_jV_k$$

\subsubsection*{Dirac-delta and Heaviside step function}
\[
\delta(x-a)=\begin{cases}
\infty, & x=a \\
0, &  x\neq a
\end{cases}\]
$$ \delta(-(x-a)) = \delta(x-a) \qquad \text{symmetric}$$
$$ \int_{-\infty}^{\infty} \delta(x-a) \diff x = 1$$
$$ \int_{-\infty}^{\infty} f(x) \delta(x-a) \diff x = f(a)$$
$$ \delta(ax) = \frac{1}{\abs{a}}\delta(x)$$
$$ \mathcal{F}\left[ \delta(x) \right] = \frac{1}{\sqrt{2\pi}}, \delta(x) = \frac{1}{2\pi}\int_{-\infty}^{\infty}e^{ikx}\diff k$$
$$ \mathcal{L}\left[ \delta(t-a) \right] = e^{-as}$$
\[
H(t-a)=\begin{cases}
0, & t<a\\
1, & t>a
\end{cases}\]
$$ H'(x-a) = \delta(x-a) $$
$$ \int_{-\infty}^{\infty} f(x) \delta^{(n)}(x-a) \diff x = (-1)^{(n)}f^{(n)}(a)$$



\begin{mdframed}
\subsection*{Calculus of variations}
\end{mdframed}
Given an integral $I=\int_{x_0}^{x_1} F(x, y, y') \diff x$ we want to find the function $x(y)$ or $y(x)$ between $(x_1, y_1)$ and $(x_2, y_2)$ such that $I$ is stationary (locally minimized). The function which satisfies this is
$$ \dv{}{x}\pdv{F}{y'} - \pdv{F}{y} = 0.$$
If $F$ has no $y$ dependence we get $\pdv{F}{y'}=\text{constant}$, which can be solved for $y$. \\
One can change variables such that we have a cyclic coordinate through the substitutions: $y' = 1/x'$ and $\diff x = x' \diff y$.
\begin{mdframed}
\subsection*{Tips and tricks}
\end{mdframed}
\begin{multline*}
  \frac{1}{z-1} = \frac{1}{z}\,\frac{1}{1-\frac{1}{z}} = \frac{1}{z}\sum_n\left(\frac{1}{z}\right)^n = \sum_n\left(\frac{1}{z}\right)^{n+1}, z>1 \\
  ax^2 + bx + c = 0 \qquad \rightarrow \qquad x_{\pm} = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} \\
  \nabla^2_{pol2D} = \pdv[2]{}{r} + \frac{1}{r}\pdv{}{r} + \frac{1}{r^2}\pdv[2]{}{\theta}\\
  \nabla^2_{spher3D} = \frac{1}{r^2}\left[ \pdv{}{r}\left( r^2\pdv{}{r} \right) + \frac{1}{\sin{\theta}}\pdv{}{\theta}\left( \sin{\theta}\pdv{}{\theta}\right) + \frac{1}{\sin^2{\theta}}\pdv[2]{}{\phi}\right]\\
  \dv[n]{}{z}\left(z-z_0\right)^{k} = \frac{k!}{(k-n-1)!}\left(z-z_0\right)^{k-n}\quad k\geq n\\
  \dv[n]{}{z}\left(z-z_0\right)^{-1} = (-1)^nn!\left(z-z_0\right)^{-(n+1)}\\
  \abs{e^{x+iy}} = e^x\\
  \sum_{k=0}^{n-1} x^k = \frac{x^n - 1}{x-1} \\
  \frac{az}{\left(z-b\right)^2} = \frac{A}{z-b} + \frac{B}{\left(z-b\right)^2} \\
  \frac{1}{ax^2+bx+c} = \frac{Ax+B}{ax^2 + bx + c}\\
  \sin^2{x} = \frac{1}{2}-\frac{1}{2}\cos{2x} \\
  \cos^2{x} = \frac{1}{2}+\frac{1}{2}\cos{2x} \\
  \int x \sin{ax} \diff x = \frac{1}{a^2}\sin{ax}-\frac{x}{a}\cos{ax}+C \\
  \int x \cos{ax} \diff x = \frac{1}{a^2}\cos{ax}+\frac{x}{a}\sin{ax}+C \\
\end{multline*}
DE with exponential solution can be written as $A\sinh{x} + B\cosh{x}$.\\
Rember that $n=0$ is a possibility when doing boudary conditions for the derivatives of a $A\sin{px} + B\cos{px}$ solution.
\subsubsection*{\small Steady-state temperature in cylinder}
Laplace equation in cylindrical coord. with assumption $u=R(r)\Theta(\theta)Z(z)$ gives
$\frac{1}{R}\frac{1}{r}\DIFF{}{r}\left(r\DIFF{R}{r}\right)+\frac{1}{\Theta}\frac{1}{r^2}\DIFF{^2\Theta}{\theta^2}+\frac{1}{Z}\DIFF{^2 Z}{z^2}=0$
$$
 \frac{1}{Z}\dv[2]{Z}{z}=K^2;
 \frac{1}{\Theta} \dv[2]{\Theta}{\theta}=-n^2;
 \frac{r}{R}\dv{}{r}\left(r\dv{R}{r}\right)-n^2+K^2 r^2=0
$$
where $R(r)$ is solved by the orthogonal bessel functions $J_n(Kr)$.
\subsubsection*{\small Steady-state final and initial condition}
Given some initial and final temperature distribution, $u_0$ and $u_f$, with fixed endpoints set to zero, we must add the final steady state distribution to our Fourier series
$$ u(x,t) = \sum_{n=1}^{\infty} A_ne^{-(n\pi\alpha/l)^2t}\sin{\left( \frac{n\pi x}{l} \right)} + u_f(x)$$
We must add this since the exponential will kill all terms as $t\rightarrow\infty$, we then find the coeffs by matching
$$ u_0-u_f = \sum_{n=1}^{\infty} A_n\sin{\left( \frac{n\pi x}{l} \right)}$$
The steady state solution means $\dot{u}(x,t)=0\rightarrow \nabla^2 u = 0$.
\subsubsection*{\small Legendre}
\textbf{Egenfunksjoner:} \\
Legendrepolynomer $y_n(x)=P_n(x)$ for $x\in[-1,1]$, $P_n(-x) = (-1)^nP_n(x)$ \\
\textbf{Orthogonalitet: }
$  \int_{-1}^1 P_n(x)P_m(x)dx=\frac{2}{2n+1}\delta_{nm}.$ \\
\textbf{Kompletthet: }
\EQU{
f(x)=\sum_{n=0}^\infty a_nP_n(x),x\in[-1,1]; \\
a_n=\frac{2n+1}{2}\int_{-1}^1 f(x) P_n(x)dx
}\\
$$ \int_0^1 P_{2n+1}(x) \diff x = (-1)^n \frac{(2n)!}{2^{2n+1}n!(n+1)!}$$\\
$$ \int_0^1 P_{2n}(x) \diff x = 0 $$

\subsubsection*{\small Hermite}
\textbf{Egenfunksjoner:}\\
Hermitepolynomer $y_n(x)=H_n(x)$ for $x\in(-\infty,\infty)$. \\
\textbf{Orthogonalitet: }
$\int_{-1}^1 e^{-x^2}H_n(x)H_m(x)dx=2^n n! \sqrt{\pi}\delta_{nm} $ \\
\textbf{Kompletthet:} \EQU{
f(x)=\sum_{n=0}^\infty a_nH_n(x),x\in(-\infty,\infty);  \
a_n=\frac{1}{2^n n! \sqrt{\pi}}\int_{-\infty}^\infty
f(x) e^{-x^2} H_n(x)dx
}

\newpage

\begin{mdframed}
\subsection*{Ordinary differential eq.}
\end{mdframed}
\textsc{Ordinary:} Derivatives with respect to only one variable.\\
\textsc{Linear:} Only linear terms $y$, $y'$ etc. no $yy'$, $y^2$, $(y'')^3$ etc.
\textsc{Order of DE:} Order of highest derivative.\\
For second order DE we get exact solution from BC since we can solve DE for $y''$ and evaluate at BC, and get higher orders from taking the derivative of this equation. From these values we can construct a Taylor series of the function, which uniquely fixes the solution.\\
A linear combination of two solutions is also a solution.\\
Criteria for two solutions $y_1(x)$ and $y_2(x)$ being linearly independent:
\begin{itemize}
  \item Linearly independent if $c_1y_1(x) + c_2y_2(x) = 0$ can only be satisfied when $c_1 = c_2=0$ ($y_2$ is not a multiple of the other).
  \item Linearly dependent if $y_2(x_0) = Ky_1(x_0)$ AND $y_2'(x_0) = Ky_1(x_0)$ they are linearly dependent.
  \item Linearly independent if $y_1'(x_0)/y_2'(x_0) = y_1(x_0)/y_2(x_0)$, in other words: if the wronskian is non-zero $$ w(x_0) = \begin{vmatrix}
  y_1(x_0)  & y_2(x_0)\\
  y_1'(x_0) & y_2'(x_0)
  \end{vmatrix} \neq 0$$
\end{itemize}
Then we can write the full solution as $y(x)=c_1y_1(x) + c_2y_2(x)$.
\subsubsection*{\small Separable solutions}
A differential equation is separable if it can be written as $$f(y) \diff y = g(x) \diff x$$
then it can be solved by integrating both sides.

\subsubsection*{\small Integrating factors}
$$\dv{y}{x} + P(x)y = Q(x) \rightarrow \diff y + \left(Py-Q\right)\diff x = 0$$
Multiply with $\mu(x)$ to get an exact solution from.
$$ \mu(x) = \exp{\int P \diff x}$$
$$ y(x) = \frac{1}{\mu(x)}\left( \int \mu(x)Q(x)\diff x +C\right)$$
\subsubsection*{\small Variation of constants}
If $y_1(x)$ is a solution we can find another linearly independent solution $y_2(x)$ by writing $y_2(x) = c(x)y_1(x)$, where we determine $c(x)$ from DE. When finding $c$ we can ignore constants since these constants contribute function equal to $y_1(x)$, and constant factors can be determined outside of solution.
\subsubsection*{\small Constant coefficients}
$$ y'' + ay' + by = 0$$
Always has an exponential solution $y=e^{\lambda x}$, inserting gives $\lambda^2 + a\lambda + b = 0$ which we solve to find $\lambda$. This gives three possibilities:
\begin{itemize}
  \item $\lambda_+ \neq \lambda_-$ and both are real: we get two linearly independent solutions $y(x) = c_1e^{\lambda_+x} + c_2e^{\lambda_-x}$.
  \item Double root $a^2-4ab=0$: gives $\lambda_+ = \lambda_- = \lambda = -a/2$. Can be shown from variation of constants that we get two linearly independent solutions $y_1(x) = e^{\lambda x}$ and $y_2(x) = xe^{\lambda x}$.
  \item Complex roots $a^2-4b<0$: gives $\lambda_{\pm} = -a/2 \pm i\sqrt{4b-a^2}/2 = -a/2 \pm iw$. Solution is the standard form, but can be written in many different forms: $y(x) = e^{-ax/2}\left(Ae^{iwx} + Be^{-iwx}\right)$\\
  $y(x) = e^{-ax/2}\left( C\cos{wx} + D\sin{wx} \right)$\\
  $y(x) = e^{-ax/2}\sin{(wx+\delta)}E$.
\end{itemize}

\subsubsection*{\small Euler-Cauchy equation}
$$ x^2y'' + a_1xy1+a_0y = 0\qquad \text{or}\qquad y''+\frac{a_1}{x}y' + \frac{a_0}{x^2}=0$$
Use substitution $x=e^{z}$ for $x>0$ and $x=-\abs{x}=-e^{z}$ for $x<0$. This gives us $\dv{y}{x}=1/x\dv{y}{z}$ and $\dv[2]{y}{x} = -1/x^2 \dv{y}{z} + 1/x^2\dv[2]{y}{z}$, inserting this into DE gives
$$ \dv[2]{y}{z} + (a_1-1)\dv{y}{z} + a_0y = 0$$
This is a differential equation with constant coefficients which we can solve, then substitute back $z=\ln{\abs{x}}$. Get different coefficients from initial conditions for positive and negative $x$, no solution for $x=0$.

\subsubsection*{Inhomogeneous DEs}
$$y'' + P(x)y' +Q(x)y = R(x)$$
The full solution is the sum of the homogeneous and particular equation $y(x)=y_{h}(x)+y_p(x)$, where $y_p$ has no undetermined coefficients. The homogeneous equation can match boundary conditions if the Wronskian is non-zero.

\subsubsection*{\small Method of undetermined coeffs.}
$$ y'' + ay' + by = R(x)$$
Method works when $R(x)$ is a function whose derivative resembles the function itself: exponential, trigonometric, polynomial, sine/cosine and combination of these. We then guess on a solution of the same form as $R(x)$, (possibly times $x$ or $x^2$) with an unknown coefficient which is determined by putting it into the equation:
\begin{itemize}
  \item $R(x) = Ae^{kx}$. We call the roots of the characteristic equation of the homogeneous equation $\alpha$ and $\beta$, then:
    \begin{itemize}
      \item $k\neq \alpha, \beta:$ $y_p(x) = Be^{kx}$
      \item $k=\alpha \text{ or } \beta$, and $\alpha\neq\beta:$ $y_p(x) = Cxe^{kx}$
      \item $k=\alpha=\beta:$ $y_p(x) = Dx^2e^{kx}$
    \end{itemize}
  \item $R(x) = A\sin{kx}$ or $R(x) = A\cos{kx}$: $y_p(x) = Ae^{ikx}$ and take real or imaginary part of solution.
  \item $R(x) = e^{kx} P_n(x)$ where $P_n(x)$ is a polynomial of degree $n$
  \begin{itemize}
    \item $k\neq \alpha, \beta:$ $y_p = e^{kx}Q_n(x)$
    \item $k=\alpha \text{ or } \beta$, and $\alpha\neq\beta:$ $y_p(x) = e^{kx}Q_n(x)x$
    \item $k=\alpha=\beta:$ $y_p(x) = e^{kx}Q_{n}(x)x^2$
  \end{itemize}
\end{itemize}
If RHS is a sum of two different such functions you take the sum of two guesses (???)

\subsubsection*{\small Finding particular solution from factorization}
$$y'' + P(x)y' + Q(x)y = R(x)$$
We are given a known solution $u(x)$ of the homogeneous DE. Guess on a particular solution $y_p(x) = u(x)v(x)$, insert into DE and use that $u$ is a solution to the homogeneous equation to reduce equation down to
$$ v'' + \left(\frac{2u'}{u}+P\right)v' = \frac{R}{u}$$
Define $w(x)=v'(x)$, getting
$$ w + \left(\frac{2u'}{u}+P\right)w = \frac{R}{u}$$
Solve this equation by integrating factors and find $v$ by $v(x) = \int w(x) \diff x$, no integration constant, get full solution by multiplying $y_p(x) = u(x)v(x)$.

\subsubsection*{\small Variation of parameters}
Know two linearly independent homogeneous solutions $y_1$ and $y_2$, the particular solution is then
$$ y_p(x) = -y_1 \int \frac{y_2(x)R(x)}{W(x)} \diff x + y_2 \int \frac{y_1(x) R(x)}{W(x)}\diff x$$
where $W$ is the Wronskian $W(x) = \begin{vmatrix} y_1(x)  & y_2(x)\\ y_1'(x) & y_2'(x) \end{vmatrix}.$ Remember to write the DE in standard form before reading of $R(x)$. \\
Derived from $y_p(x) = f_1(x)y_1(x) + f_2(x)y_2(x)$ and impose condition $f_1'y_1 + f_2'y_2 = 0$ to make terms in $y_p'$ vanish. Insert this into DE and use that $y_1$ and $y_2$ satisfy homogeneous equation to make two terms disappear. This gives us one equation, which in combination with out imposed condition ($f_1'y_1 + f_2'y_2 = 0$) gives us the expression for $f_1$ and $f_2$ which we use to find $y_p$ from $y_p(x) = f_1(x)y_1(x) + f_2(x)y_2(x)$.

\subsubsection*{\small Greens functions}
$$ \left(\underbrace{\dv[2]{}{x} + P(x)\dv{}{x} + Q(x)}_{D}\right)y = R(x) $$
Assume given DE, homogeneous solution, and BC. LHS and BCs give us the Greens function $G(x,z)$ which gives us $y_h + y_p$ for any $R(x)$ with no arbitrary constants. Begin by writing DE as $Dy(x)=R(x)$, exists $D^{-1}$, then $y(x) = D^{-1}R(x)$, this is an integral $$y(x) = \int_a^b G(x,z)R(z)\diff z, \qquad a \leq x, z \leq b$$
We apply $D_x$ to both sides, giving us
$$D_xy(x) = \int_a^b \underbrace{D_xG(x,z)}_{\delta(x-z)}R(z)\diff z$$
Have to solve
$$ D_xG(x,z) = \delta(x-z)$$
The Greens function satisfy the original DE, but with $R(x) = \delta(x-z)$. We have restrictions on $G(x,z)$
\begin{itemize}
  \item $G(x,z)$ must satisfy the given BCs given for $y(x)$.\\ Ex: $y(a)=y(b)=0 \rightarrow G(a,z) = G(b,z)=0$
  \item $G(x,z)$ is continuous at $x=z$.
  \item We have a singularity at $x=z$, we integrate the DE for $G(x,z)$ from $z-\epsilon$ to $z+\epsilon$, this gives us that $\dv{G(x,z)}{x}\Big|_{z-\epsilon}^{z+\epsilon} = 1$
\end{itemize}
Begin with $DG(x,z)=\delta(x-z)$. For $x<z$ and $x>z$ we get different solutions, which both are equal to the homogeneous solution, but their coefficients are different and dependent on $z$. Use the boundary conditions, the continuity of $G$ at $x=z$, and the discontinuity of it's derivative $\dv{G(x,z)}{x}\Big|_{z-\epsilon}^{z+\epsilon} = 1$ to find $G(x,z)$ for $x<z$ and $x>z$ and solve the integral for $y(x)$, which will be two integrals
$$y(x) = \int_a^x G_1(x,z)R(z) \diff z + \int_x^b G_2(x,z)R(z) \diff z$$
where $G_1$ is the greens function for $z<x$ and $G_2$ is for $z>x$.

\subsubsection*{\small Power series}
$$ y'' + P(x)y' + Q(x)y = 0$$
Useful for DEs whose solution can not be written in terms of elementary functions. \\
Represent $P(x)$ and $Q(x)$ as power series (if they are not already polynomials), and assume solution on the form
$$ y(x) = \sum_{n=1}^{\infty} a_n x^n, \qquad \quad y'(x) = \sum_{n=1}^{\infty} na_n x^{n-1}$$ $$y''(x) = \sum_{n=1}^{\infty} n(n-1)a_n x^{n-2} = \sum_{n=1}^{\infty} n(n+1)a_{n+1} x^{n-1}$$
Insert into DE and equate power by power in $x$ to determine coefficients $a_n$.\\
\textsc{Existence of solution:} If $P(x)$ and $Q(x)$ are analytic at $x=x_0$, then every solution of the differential equation is analytic at $x=x_0$ and thus can be represented by a power series in $(x-x_0)$ with some radius of convergence $R>0$.
$$ \text{Legendre functions: } (1-x^2)y'' -2xy' + l(l+1)y = 0$$


\subsubsection*{\small Fröbenious method}
$$y'' + \frac{b(x)}{x}y' + \frac{c(x)}{x^2}y = 0 $$
where $b(x)$ and $c(x)$ are analytic at $x=0$, has at least one solution that can be represented as
$$y(x) = x^s \sum_{n=1}^{\infty} a_n x^n$$
By construction $a_0 \neq 0$, and $s$ may be non-integer, real or complex.
Determine $s$ from matching terms for lowest possible power, this gives \textbf{indicial equation}, which is a 2nd order equation for $s$ with at least one solution. Write DE as
$$ x^2y'' + xb(x)y' + c(x)y = 0$$
with $b$ and $c$ written as polynomials or infinite series, then equate terms with $x^s$ which gives \textsc{indicial equation}, this gives three possible scenarios:
\begin{itemize}
  \item Two different roots, and their difference is not an integer. Then we have two linearly independent solutions.
  \item Two different roots, and their difference is an integer. Find coefficients for the smallest root first, since it might give the full solution. Get full solution if $a_1$ is undetermined, as well as $a_0$ which is always undtermined. If it does not give the full solution then finding the coefficients with the other root will. Sometimes the the smallest root does not give a solution, then find solution of largest root and use variation of constants $y_2(x) = f(x)y_1(x)$.
  \item Double root. Find coefficient with this root, and find second solution by variation of constants $y_2(x) = f(x)y_1(x)$.
\end{itemize}

\textbf{Fuchs' theorem:}
A differential equation $y'' + f(x)y' + g(x)y = 0$ has a non-essential singularity at the origin if $xf(x)$ and $x^2g(x)$ is expandable in convergent power series. The solution will then either be (1) two Frobenius series, or (2) one solution $S_1(x)$ and another $S_1(x)\ln{x}+S_2(x)$ where $S_1$ and $S_2$ are the Frobenius solutions.
\newpage
\begin{mdframed}
\subsection*{Fourier series}
\end{mdframed}
Series representation of periodic functions expanded in $\sin{}$, $\cos{}$ or $e$ rather than in power series. Can represent functions with cusps and discontinuities.\\ Orthogonality relations:
$$ \int_{-\pi}^{\pi}\cos{nx} \diff x = \int_{-\pi}^{\pi}\sin{nx} \diff x = 0\qquad \text{$n$ is integer $\neq 0$} $$
$$ \int_{-\pi}^{\pi}\cos{nx}\sin{mx} \diff x = 0 \qquad \text{$n$, $m$ is integer} $$
$$ \frac{1}{2\pi}\int_{-\pi}^{\pi}\sin{nx}\sin{mx} \diff x = \begin{cases}
0; & m\neq n \\
1/2; & m = n \neq 0 \\
0; & $m,m = 0$ \\
\end{cases}$$
$$ \frac{1}{2\pi}\int_{-\pi}^{\pi}\cos{nx}\cos{mx} \diff x = \begin{cases}
0; & m\neq n \\
1/2; & m = n \neq 0 \\
1; & m,m = 0 \\
\end{cases}$$

\textbf{Period of $2\pi$:}
$$ f(x) = \frac{1}{2}a_0 + \sum_{n=1} a_n\cos{nx} + \sum_{n=1} b_n\sin{nx}$$
Find coefficients by using orthogonality of sin and cos
$$a_n = \frac{1}{\pi} \int_{-\pi}^{\pi} f(x)\cos{nx} \diff x \quad b_n = \frac{1}{\pi} \int_{-\pi}^{\pi} f(x)\sin{nx} \diff x $$
Can also write as exponentials:
$$f(x) = \sum_{n=-\infty}^{\infty} c_n e^{inx}, \quad c_n = \frac{1}{2\pi}\int_{-\pi}^{\pi} f(x)e^{-inx} \diff x$$


\textbf{Dirichlets:}\\
If $f(x)$ is periodic with period of $2\pi$ between $-\pi$ and $\pi$, it is single valued, has a finite number of maximum and minimum values, finite number of discontinuities, $\int_{-\pi}^{\pi} \abs{f(x)}\diff x$ is finite, then the Fourier series converges. For jumps the Fourier series converges to the midpoint.

\textbf{Other basic intervals:}\\
For basic interval of length $2l$, $[-l, l]$, we can do the transform $x\rightarrow n\pi/l$ and $\frac{1}{\pi}\int_{-\pi}^\pi \rightarrow \frac{1}{l}\int_{-l}^l$.
$$ a_n = \frac{1}{l}\int_{-l}^l f(x) \cos{\left(\frac{n\pi x}{l}\right)} \diff x$$
$$ b_n = \frac{1}{l}\int_{-l}^l f(x) \sin{\left(\frac{n\pi x}{l}\right)} \diff x$$
$$ c_n = \frac{1}{2l}\int_{-l}^l f(x) \exp{\frac{-in\pi x}{l}} \diff x$$
For basic interval of length $2l$ on $[0, 2l]$ or $[3l, 5l]$, we only change the integration limits to that same interval, while the form of the coefficients is the same.\\
For basic interval $[0, l]$ we can find:
\begin{itemize}
  \item a series with periodicity of $l$ instead of $2l$, meaning $l\rightarrow l/2$: $a_n = \frac{1}{l/2}\int_0^l f(x)\cos{\left( \frac{n\pi x}{l/2} \right)}$.
  \item Define an even extension to a periodicity of $2l$, this gives a cos-series.
  \item Define an odd extension of the period to a length of $2l$, this gives a sine-series.
\end{itemize}
\textbf{Even functions:} $f(-x)=f(x)$\\
\textbf{Odd  functions:} $f(-x)=-f(x)$\\
odd $\cdot$ odd $=$ even $\cdot$ even $=$ even\\
even $\cdot$ odd $=$ odd $\cdot$ even $=$ odd
\begin{equation*} \int_{-L}^{L}f(x) \diff x = \begin{cases}
0; & f(x) \text{ is odd} \\
2\int_0^L f(x) \diff x; & f(x) \text{ is even}
\end{cases} \end{equation*}
The Fourier series of an even function of period $2l$ is a cosine series
$$f(x)= \frac{a_0}{2} + \sum_{n=1}^{\infty} a_n \cos{\left( \frac{n\pi x}{l} \right)},\quad a_n = \frac{2}{l}\int_0^lf(x)\cos{\left( \frac{n\pi x}{l} \right)}\diff x $$
The Fourier series of an odd function of period $2l$ is a sine series ($f(0)=0$ when $f$ is odd)
$$f(x)= \sum_{n=1}^{\infty} b_n \sin{\left( \frac{n\pi x}{l} \right)},\quad b_n = \frac{2}{l}\int_0^lf(x)\sin{\left( \frac{n\pi x}{l} \right)}\diff x $$

\textbf{Perseval's theorem:}
For a complex Fourier series $f(x)$ we can evaluate infinite series. Can use complex or real Fourier series
$$ \frac{1}{2l}\int_{-l}^{l} \abs{f(x)}^2 = \sum_{n=-\infty}^{\infty} \abs{c_n}^2$$
$$ \frac{1}{2l}\int_{-l}^{l} \abs{f(x)}^2 = \left(\frac{a_0}{2}\right)^2 + \frac{1}{2}\sum_{n=1}^{\infty} a_n^2 + \frac{1}{2}\sum_{n=1}^{\infty} b_n^2$$
Double orthogonality:
$$\int_0^l\int_0^l \sin{n\pi x/l}\sin{m\pi y l}\sin{p\pi x/l}\sin{q\pi y/l} \diff x \diff y = \begin{cases}
1/4; & n=p, m=q \\
0;   & \text{otherwise} \end{cases}$$
\begin{mdframed}
\subsection*{Fourier integrals}
\end{mdframed}
$F(k)$ is the Fourier transform of $f(x)$, and $f(x)$ is the inverse Fourier transform of $F(k)$.
$$ f(x) = \mathcal{F}^{-1}\left[ F(k) \right] = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} F(k) e^{ikx} \diff k $$
$$ F(k) = \mathcal{F}\left[ f(x) \right] = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} f(x) e^{-ik x} \diff x$$
The sign in the exponent and the $1/\sqrt{2\pi}$ is a convention.\\
Ok to use Fourier integrals if: $f(x)$ satisfy Dirichlet conditions on any finite interval, $\int_{-\infty}^{\infty} \abs{f(x)} \diff x$ is finite, at discontinuities in $f(x)$ this formula gives the midpoint of the jump.\\
The Fourier transform of a Gaussian is a Gaussian.\\
Fourier integrals be used to go from a DE to an algebraic equation which you can solve for $\mathcal{F}\left[ f(x) \right]$ and take the inverse Fourier transform
$$ \mathcal{F}\left[ f^{(n)}(x) \right] = (ik)^n\mathcal{F}\left[ f(x) \right]$$
\textbf{Perseval's theorem (v2):}
For the Fourier transform $F(k)$ of $f(x)$ satisfying Dirichlet conditions we have
$$\int_{-\infty}^{\infty} \abs{f(x)}^2 \diff x = \int_{-\infty}^{\infty} \abs{F(k)}^2 \diff k $$
\textbf{Even and odds:}
Even functions are represented by cos-transform\\
Odd  functions are represented by sin-transform\\
If $f(x)$ is even/odd then $F(k)$ is even/odd. Making the Fourier transform for even and odd functions respectively:
$$F(k) = \sqrt{\frac{2}{\pi}}\int_0^\infty f_e(x) \cos{kx}\diff x, f_e(x) = \sqrt{\frac{2}{\pi}} \int_0^\infty F(k)\cos{kx} \diff k$$
$$F(k) = \sqrt{\frac{2}{\pi}}\int_0^\infty f_o(x) \sin{kx}\diff x, f_o(x) = \sqrt{\frac{2}{\pi}} \int_0^\infty F(k)\sin{kx} \diff k$$

\begin{mdframed}
\subsection*{Laplace transforms}
\end{mdframed}
An integral transform which reduces DE to algebraic equations where initial value problems are solved directly. The Fourier transform of $f(t)$ is written as $F(s)$ and is defined as
$$ F(s) = \mathcal{L}\left[ f(t) \right] = \int_0^\infty e^{-st} f(t) \diff t \qquad f(t) = \mathcal{L}^{-1}\left[ F(s) \right]$$
Inverse Laplace transforms are found from tables
$$ \mathcal{L}\left[ f^{(n)}(t) \right] = s^n\mathcal{L}\left[ f(t) \right]-s^{n-1}f(0)-s^{n-2}f'(0) - ...-f^{(n-1)}$$
$$ \text{s-shifting: }\mathcal{L}^{-1}\left[ F(s-a) \right] = e^{at}\mathcal{L}^{-1}\left[ F(s)\right] $$
$$ \text{t-shifting: }\mathcal{L}^{-1}\left[ e^{-as}F(s) \right] = f(t-a)\underbrace{H(t-a)}_{\text{Heaviside}}$$
$$ \mathcal{L}^{-1}\left[ H(t-a) \right] = \frac{1}{s}e^{-as}$$
$$ \mathcal{L}\left[ f(t)\cdot t \right] = -F'(s) \text{  or  } \mathcal{L}^{-1}\left[F'(s)\right] = -tf(t)$$
\textbf{Convolution:}
We are given $H(s) = F(s)G(s)$ and know $f(t) = \mathcal{L}^{-1}\left[F(s)\right]$ and $g(t) = \mathcal{L}^{-1}\left[G(s)\right]$, and want to use tihs to find $h(t) = \mathcal{L}^{-1}\left[H(s)\right]$ in terms of $f(t)$ and $g(t)$, we call this the \textsc{convolution} of $f$ and $g$, and is written as, and can be found by
$$h(t) = \mathcal{L}^{-1}\left[H(s)\right] = \left( f*g\right)(t) = \int_0^t f(\tau)g(t-\tau) \diff \tau$$
it does not matter which function is $f$ and which is $g$, choose $g$ as the simplest function of the two $f*g=g*f$ and $\mathcal{L}\left[f*g\right]=\mathcal{L}\left[f\right]\mathcal{L}\left[g\right]$.\\
$$1\rightarrow 1/s, e^{-at} \rightarrow \frac{1}{s+a}, \sin{at}\rightarrow\frac{a}{s^2+a^2}\rightarrow \cos{at}\rightarrow \frac{p}{p^2+a^2} $$\\
$$t^k \rightarrow \frac{k!}{s^{k+1}}, t^ke^{-at}\rightarrow \frac{k!}{\left(p+a\right)^{k+1}} $$
\textbf{Bromwich integral:}
Laplacetransform can be related to fourier transform by allowing $s$ to be complex, $s\rightarrow x+iy$, and can then find an explicit formula for the inverse laplace transform
$$ f(t) = \frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty} F(z)e^{zt} \diff z, \quad t>0$$
Can solve integral by finding residues of all singularities of the integrand. Close contour with a semi-circle with no contribution as $R\rightarrow \infty$ and get $f(t) = \sum \text{res}(F(z)e^{zt})$.


\begin{mdframed}
\subsection*{Partial DE}
\end{mdframed}
\subsubsection*{Separation of variables}
Transform a PDE with $n$ variables to $n$ ordinary DE. Find a solution of those DEs satisfying boundary conditions, this gives infinitely many solutions, this is not the most general solution but forms a basis of solutions which we can expand the full solution in. Superposition of these such that initial conditions are satisfied (a Fourier series).\\
Write $u(x,t)$ as $F(x)$ or $G(t)$. Use separation of variable and solve their ordinary DE separately. Quantize constant such that they can satisfy BCs. This gives
$$ u(x,t) = \sum_{n=1}^{\infty} A_n f_n(x)g_n(t)$$
and impose initial conditions, which should give a Fourier series f.ex
$$ u(x,0) = \sum_{n=1}^\infty A_n \sin{\left( \frac{n\pi x}{L} \right)} \rightarrow A_n = \frac{2}{L}\int_0^L u(x,0) \sin{\left( \frac{n\pi x}{L} \right)} \diff x$$
\textbf{1D wave equation:} $\pdv[2]{u}{t} = c^2\pdv[2]{u}{x}$, $u(x,t) = F(x)G(t)$, get $F'' = kF$ and $\ddot{G} = kc^2G$. To satisfy BC ($u(0,t) = u(L,t)=0$): $k=-p^2$, gives $F_n = \sin{\left(p_nx\right)}$ and $G_n = A_n \cos{p_nc t} + B_n\sin{p_nc t}$ with $p_n=n\pi/L$.
Combine to get $u_n(x,t) = \left(A_n \cos{p_nc t} + B_n\sin{p_nc t}\right)\sin{p_nx}$, where $A_n$ and $B_n$ are fixed by initial conditions: $u(x,0) = f(x) = \sum_{n=1}^{\infty}A_n \sin{p_n t} \rightarrow A_n = 2/L \int_0^Lf(x)\sin{p_nx}\diff x$. And $\dot{u}(x,0)=g(x)=\sum_{n=1}^{\infty}\lambda_nB_n\sin{p_nx} \rightarrow B_n\lambda_n = 2/L \int_0^L g(x)\sin{p_nx}\diff x$ \\
\textbf{1D heat equation:} $\pdv{u}{t} = c^2\pdv[2]{u}{x}$, $u(x,t) = F(x)G(t)$, get $F'' = kF$ and $\dot{G} = kc^2G$, to satisfy BC $k=-p^2$, gives $F=A_n\cos{p_nx}+B_n\sin{p_nx}$ and $G = e^{-c^2p_n^2t}$. Impose BC $u_n(x,t) = B_n\sin{p_nx}e^{-c^2p_n^2t}$. Use initial conditions $B_n=\frac{2}{L}\int_0^Lu(x,0)\sin{n\pi x/L}$.\\
\textbf{2D wave equation:} First separate time and space $u = F(x,y)G(t)$, then separate $F(x,y) = H(x)Q(y)$. Get $\ddot{G}=-c^2\nu^2G$, $H'' = -k^2H$, $Q'' = -p^2Q$ with $p^2+k^2 = \nu^2$. Get $F_{n}(x,y) = \sin{n\pi x/a}\sin{m\pi x/b}$. We get a double Fourier series
$u(x,y,t) = \sum_{n,m=1}^{\infty}[\alpha_{mn}\cos{cp_{nm}t} + \beta_{mn}\sin{p_{nm}ct}]\sin{n\pi x/a}\sin{m\pi x/b}$. If $u(x,y,0) = f(x,y)$ we get
$\alpha_{mn} = \frac{4}{\alpha\beta}\int_0^a\int_0^b f(x,y)\sin{n\pi x/a}\sin{m\pi x/b} \diff x \diff y$

\subsubsection*{Integral transforms}
\textbf{Laplace transform:}
Laplace transform wrt. one of the variables, and we get an ordinary DE for the other variable which we solve to find the Laplace transform of the solution, which we can take the inverse transform of to find the actual solution.\\
If we have a function $u(x, t)$ and take the Laplace transform wrt. $t$ we use $\mathcal{L}\left[\pdv{u}{x}\right]=\pdv{U}{x}$ and $\mathcal{L}\left[\pdv{u}{t}\right] = sU - u(x,0)$. Solve ordinary DE, and impose initial conditions $\mathcal{L}\left[u(x,0)\right] = U(0, s)$. When $U(x,s)$ is found the inverse Laplace transform is done.\\
\textbf{Fourier transform:}
Fourier transform with respect to one of the variables (in this case $x$), and use $\mathcal{F}\left[\pdv[2]{u}{x}\right] = -k^2U(k,t)$. Impose initial conditions $U(k,0)=\mathcal{F}\left[u(x,0)\right]$, solve the ordinary DE for $U(k,t)$ and take the inverse transform.

\subsubsection*{Orthogonal functions}
Differential equation with certain BCs have set of solutions that are orthogonal and can expand any function, this goes for DE on the form
$$ P(x)y'' + P'(x)y' + \left[q(x) + \lambda r(x)\right]y = 0 $$
with some BC at $x=a$ and $x=b$ and $r(x)>0$ on $[a,b]$. If there exists a non-zero solution for a given $\lambda$ the $y(x)$ is called an eigenfunction, and $\lambda$ is called the eigenvalue of the problem. Typically BCs lead to a series of discrete eigenvalues $\lambda_m$ and eigenfunctions $y_m(x)$. Examples: (1) Fourier $y'' + (n\pi/L)^2y=0$, (2) Legendre $(1-x^2)y'' - 2xy'+n(n+1)y = 0$. The eigenfunctions being orthogonal means
$$ \int_a^br(x)y_n(x)y_m^*(x)\diff x = 0 $$
and we use this orthogonality relation to expand any function through the eigenfunctions by $f(x) = \sum_{n=1}^\infty a_n y_n(x)$ by multiplying each side with $r(x)y_m^*(x)$ and integrating from $a$ to $b$, giving us
$$ a_m = \int_a^b f(x)r(x)y_m^*(x) \diff x$$
$D$ is hermitian if it satisfies
\[
\int_{a}^{b}y_n(x)^*D y_m(x) dx = \int_{a}^{b} y_m(x) D y_n(x)^* dx
\]
for boundary conditions, then: real eigenvalues, eigenfunctions are orthogonal over $a,b$, and the eigenfunctions forms a complete sett for $a,b$. Where $D= p(x)\pdv[2]{}{x}+p'(x)\pdv{}{x} +q(x)$.\\
Examples:\\
\textbf{Legendre:} $x\in[-1,1]$,
$(1-x^2)y''-2xy'+n(n+1)y=0$ \\
where $\lambda=n(n+1), p(x)=1-x^2, q(x)=0, r(x)=1$. \\
\textbf{Fourier:}  $x\in[-L,L]$,
$y''+(n\pi/L)^2y=0$\\
where $\lambda=(n \pi/L)^2, p(x)=1, q(x)=0, r(x)=1$. \\
\textbf{Hermite:} $x\in(-\infty,\infty)$
$y''-2xy'+2ny=0$ \\multiply with $e^{-x^2}$
$e^{-x^2}y''-2xe^{-x^2}y'+2ne^{-x^2}y=0$ \\
where $\lambda=2n, p(x)=e^{-x^2}, q(x)=0, r(x)=e^{-x^2}$. \\
\textbf{Laguerre:} $x\in[0,\infty)$.\\
$xy''+(1-x)e^{-x}y'+\lambda e^{-x} y=0$ multiply with $e^{-x}$\\
$xe^{-x}y''+(1-x)e^{-x}y'+\lambda e^{-x}y = 0$ \\
where $p(x)=xe^{-x}, q(x)=0, r(x)=e^{-x}$. \\


\end{multicols*}
\end{document}
